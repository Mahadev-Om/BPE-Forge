{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##BPE Trainer"
      ],
      "metadata": {
        "id": "4Cv0Fxw4pd-s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BGZRC1uelHqp"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "strings=[\"How are you doing?\",\"How's the weather today?\",\"What is your favourite sport?\",\"What plans do you have after the midterm?\",\"How many projects do you want to finish in the midterm?\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\",\"[MASK]\"])"
      ],
      "metadata": {
        "id": "YURQqqZIlcJH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "zip_file_path = \"/content/wikitext-103-raw (3).zip\"\n",
        "\n",
        "# Directory to extract the files to\n",
        "extract_dir = \"data/wikitext-103-raw (3)\"\n",
        "\n",
        "# Extract the files\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# File paths after extraction\n",
        "files =[f\"{extract_dir}/wiki.{split}.raw\" for split in [\"test\",\"train\",\"valid\"]]\n",
        "\n",
        "# Check if files exist (optional, but recommended)\n",
        "for file in files:\n",
        "    if not os.path.exists(file):\n",
        "        print(f\"Error: File not found - {file}\")\n",
        "\n",
        "# Only proceed with training if all files exist\n",
        "if all(os.path.exists(file) for file in files):\n",
        "    tokenizer.train(files, trainer)"
      ],
      "metadata": {
        "id": "O173SDo7lf43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for string in strings:\n",
        "    # Encode each string individually\n",
        "    encoded_string = tokenizer.encode_batch([string])\n",
        "    print(encoded_string)"
      ],
      "metadata": {
        "id": "jOHH9uSZljqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WordLevel Model\n"
      ],
      "metadata": {
        "id": "mFii9rCAqJGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.models import WordLevel\n",
        "tokenizer1 = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "strings=[\"How are you doing?\",\"How's the weather today?\",\"What is your favourite sport?\",\"What plans do you have after the midterm?\",\"How many projects do you want to finish in the midterm?\"]"
      ],
      "metadata": {
        "id": "B_q6jzTdqNst"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.trainers import WordLevelTrainer\n",
        "trainer1 = WordLevelTrainer(special_tokens=[\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\",\"[MASK]\"])"
      ],
      "metadata": {
        "id": "qVgzOdCIrF0J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "zip_file_path = \"/content/wikitext-103-raw (3).zip\"\n",
        "\n",
        "# Directory to extract the files to\n",
        "extract_dir = \"data/wikitext-103-raw (3)\"\n",
        "\n",
        "# Extract the files\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# File paths after extraction\n",
        "files =[f\"{extract_dir}/wiki.{split}.raw\" for split in [\"test\",\"train\",\"valid\"]]\n",
        "\n",
        "# Check if files exist (optional, but recommended)\n",
        "for file in files:\n",
        "    if not os.path.exists(file):\n",
        "        print(f\"Error: File not found - {file}\")\n",
        "\n",
        "# Only proceed with training if all files exist\n",
        "if all(os.path.exists(file) for file in files):\n",
        "    tokenizer1.train(files, trainer1)"
      ],
      "metadata": {
        "id": "58KYTFhzrR8B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for string in strings:\n",
        "    # Encode each string individually\n",
        "    encoded_string = tokenizer1.encode_batch([string])\n",
        "    print(encoded_string)\n",
        "\n",
        "for encoding in encoded_string:\n",
        "    print(encoding.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ohi0alR3ruu-",
        "outputId": "0dae0db7-2a99-4a5d-a548-eaa12b4812b6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Encoding(num_tokens=1, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
            "[Encoding(num_tokens=1, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
            "[Encoding(num_tokens=1, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
            "[Encoding(num_tokens=1, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
            "[Encoding(num_tokens=1, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
            "['[UNK]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WordPiece Model"
      ],
      "metadata": {
        "id": "zDKD-IoJwX1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.models import WordPiece\n",
        "tokenizer2 = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "strings=[\"How are you doing?\",\"How's the weather today?\",\"What is your favourite sport?\",\"What plans do you have after the midterm?\",\"How many projects do you want to finish in the midterm?\"]"
      ],
      "metadata": {
        "id": "VDlf34W3sFcD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.trainers import WordPieceTrainer\n",
        "trainer2 = WordPieceTrainer(special_tokens=[\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\",\"[MASK]\"])"
      ],
      "metadata": {
        "id": "OOFWf1V5wiR8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "zip_file_path = \"/content/wikitext-103-raw (3).zip\"\n",
        "\n",
        "# Directory to extract the files to\n",
        "extract_dir = \"data/wikitext-103-raw (3)\"\n",
        "\n",
        "# Extract the files\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# File paths after extraction\n",
        "files =[f\"{extract_dir}/wiki.{split}.raw\" for split in [\"test\",\"train\",\"valid\"]]\n",
        "\n",
        "# Check if files exist (optional, but recommended)\n",
        "for file in files:\n",
        "    if not os.path.exists(file):\n",
        "        print(f\"Error: File not found - {file}\")\n",
        "\n",
        "# Only proceed with training if all files exist\n",
        "if all(os.path.exists(file) for file in files):\n",
        "    tokenizer2.train(files, trainer2)"
      ],
      "metadata": {
        "id": "pnNmX1KJwzBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for string in strings:\n",
        "    # Encode each string individually\n",
        "    encoded_string = tokenizer2.encode_batch([string])\n",
        "    print(encoded_string)\n",
        "\n",
        "for encoding in encoded_string:\n",
        "    print(encoding.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "MmBv8Lw4w-Wb",
        "outputId": "7100de3a-c649-4f92-af98-e14b6df4a28a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5c2438e078bb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Encode each string individually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mencoded_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to my observation, the BPE Model and the WordPiece Model crashed , while the Word Level Model had the unknown subword tokenization output.\n",
        "\n",
        "As, for me out of 3, 2 of the models crashed so, not possible for concluding as to which model works the best."
      ],
      "metadata": {
        "id": "zZMMihXsyo7w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bakwc4fa2wI0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}